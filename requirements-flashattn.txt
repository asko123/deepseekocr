# Optional: Install flash-attn for faster performance
# Only install if you have:
# - NVIDIA GPU with CUDA support
# - CUDA 11.8 or compatible version
# - Sufficient build tools (gcc, etc.)

# Try installing with:
# pip install --user flash-attn==2.7.3 --no-build-isolation

# If installation fails, you can run the pipeline with --no-flash-attn flag
# The model will work fine with standard attention (just slightly slower)

flash-attn==2.7.3

